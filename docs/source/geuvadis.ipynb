{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genvarloader as gvl\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import seqpro as sp\n",
    "import pooch\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Geuvadis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll see how to use GenVarLoader (GVL) to:\n",
    "\n",
    "1. Write a GVL dataset to disk\n",
    "2. Inspect the dataset\n",
    "3. Optional: write transformed versions of the tracks to disk\n",
    "4. Add on-the-fly transformations\n",
    "5. Obtain splits from the dataset\n",
    "6. Get a PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Geuvadis dataset is 451 individuals from the 1000 Genomes Project that have both whole genome sequencing and RNA-seq from blood samples. We'll see how to use GVL to get a high performance dataloader that yields haplotypes and tracks for training or running inference with sequence models. For the sake of this tutorial, we'll only work with chromosome 22 so everything can run in a few minutes.\n",
    "\n",
    "Downloading this data should take ~5-10 minutes and is the slowest step in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data from 'https://ftp.ensembl.org/pub/release-112/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.22.fa.gz' to file '/carter/users/dlaub/.cache/pooch/edfb24b9fee5f1060c26e092a696e447-Homo_sapiens.GRCh38.dna.chromosome.22.fa.gz'.\n",
      "100%|█████████████████████████████████████| 11.4M/11.4M [00:00<00:00, 14.8GB/s]\n",
      "Downloading data from 'doi:10.5281/zenodo.13656224/1kGP.chr22.pgen' to file '/carter/users/dlaub/.cache/pooch/1kGP.chr22.pgen'.\n",
      "100%|█████████████████████████████████████| 20.4M/20.4M [00:00<00:00, 17.7GB/s]\n",
      "Downloading data from 'doi:10.5281/zenodo.13656224/1kGP.chr22.psam' to file '/carter/users/dlaub/.cache/pooch/1kGP.chr22.psam'.\n",
      "100%|█████████████████████████████████████| 6.38k/6.38k [00:00<00:00, 8.98MB/s]\n",
      "Downloading data from 'doi:10.5281/zenodo.13656224/1kGP.chr22.pvar' to file '/carter/users/dlaub/.cache/pooch/1kGP.chr22.pvar'.\n",
      "100%|█████████████████████████████████████| 1.17G/1.17G [00:00<00:00, 1.09TB/s]\n",
      "Downloading data from 'doi:10.5281/zenodo.13656224/bw_chr22.tar.gz' to file '/carter/users/dlaub/.cache/pooch/c91b5d0155f29d91dddf23901e396774-bw_chr22.tar.gz'.\n",
      "100%|█████████████████████████████████████| 1.01G/1.01G [00:00<00:00, 1.35TB/s]\n",
      "Untarring contents of '/carter/users/dlaub/.cache/pooch/c91b5d0155f29d91dddf23901e396774-bw_chr22.tar.gz' to '/carter/users/dlaub/.cache/pooch/c91b5d0155f29d91dddf23901e396774-bw_chr22.tar.gz.untar'\n",
      "Downloading data from 'doi:10.5281/zenodo.13656224/bigwig_table.csv' to file '/carter/users/dlaub/.cache/pooch/943cc42de0153b9ace8f53b274ba7616-bigwig_table.csv'.\n",
      "0.00B [00:00, ?B/s]\n",
      "Downloading data from 'doi:10.5281/zenodo.13656224/chr22_egenes.bed' to file '/carter/users/dlaub/.cache/pooch/99949e386019e2a73aa81c2caf793b77-chr22_egenes.bed'.\n",
      "100%|█████████████████████████████████████| 7.26k/7.26k [00:00<00:00, 9.40MB/s]\n"
     ]
    }
   ],
   "source": [
    "# GRCh38 chromosome 22 sequence\n",
    "reference = pooch.retrieve(\n",
    "    url=\"https://ftp.ensembl.org/pub/release-112/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.22.fa.gz\",\n",
    "    known_hash=\"sha256:974f97ac8ef7ffae971b63b47608feda327403be40c27e391ee4a1a78b800df5\",\n",
    "    progressbar=True,\n",
    ")\n",
    "!gzip -dc {reference} | bgzip > {reference[:-3]}.bgz\n",
    "reference = reference[:-3] + \".bgz\"\n",
    "\n",
    "# PLINK 2 files\n",
    "variants = pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/1kGP.chr22.pgen\",\n",
    "    known_hash=\"md5:31aba970e35f816701b2b99118dfc2aa\",\n",
    "    progressbar=True,\n",
    "    fname=\"1kGP.chr22.pgen\",\n",
    ")\n",
    "pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/1kGP.chr22.psam\",\n",
    "    known_hash=\"md5:eefa7aad5acffe62bf41df0a4600129c\",\n",
    "    progressbar=True,\n",
    "    fname=\"1kGP.chr22.psam\",\n",
    ")\n",
    "pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/1kGP.chr22.pvar\",\n",
    "    known_hash=\"md5:5f922af91c1a2f6822e2f1bb4469d12b\",\n",
    "    progressbar=True,\n",
    "    fname=\"1kGP.chr22.pvar\",\n",
    ")\n",
    "\n",
    "# BigWigs and sample ID mapping\n",
    "bw_paths = pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/bw_chr22.tar.gz\",\n",
    "    known_hash=\"md5:14bf72e9e9d3e2318d07315c4a2675fb\",\n",
    "    progressbar=True,\n",
    "    processor=pooch.Untar(),\n",
    ")\n",
    "bw_table_path = pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/bigwig_table.csv\",\n",
    "    known_hash=\"md5:7fe7c55b61c7dfa66cfd0a49336f3b08\",\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "# BED\n",
    "bed_path = pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/chr22_egenes.bed\",\n",
    "    known_hash=\"md5:ccb55548e4ddd416d50dbe6638459421\",\n",
    "    progressbar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the GVL dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll specify a path to store the dataset, which is a directory (like Zarr stores if you're familiar with those)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = \"geuvadis.chr22.gvl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need a table or dictionary specifying the sample names for each BigWig. We'll use a table here, which must have at least have columns `sample` and `path` as seen below. The join is added here to update the paths to match the actual download paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sample</th><th>read_count</th><th>path</th></tr><tr><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;HG00236&quot;</td><td>34548283</td><td>&quot;/carter/users/dlaub/.cache/poo…</td></tr><tr><td>&quot;HG00259&quot;</td><td>53041143</td><td>&quot;/carter/users/dlaub/.cache/poo…</td></tr><tr><td>&quot;NA20519&quot;</td><td>36620358</td><td>&quot;/carter/users/dlaub/.cache/poo…</td></tr><tr><td>&quot;NA20811&quot;</td><td>24398971</td><td>&quot;/carter/users/dlaub/.cache/poo…</td></tr><tr><td>&quot;NA20768&quot;</td><td>30019566</td><td>&quot;/carter/users/dlaub/.cache/poo…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────────┬────────────┬─────────────────────────────────┐\n",
       "│ sample  ┆ read_count ┆ path                            │\n",
       "│ ---     ┆ ---        ┆ ---                             │\n",
       "│ str     ┆ i64        ┆ str                             │\n",
       "╞═════════╪════════════╪═════════════════════════════════╡\n",
       "│ HG00236 ┆ 34548283   ┆ /carter/users/dlaub/.cache/poo… │\n",
       "│ HG00259 ┆ 53041143   ┆ /carter/users/dlaub/.cache/poo… │\n",
       "│ NA20519 ┆ 36620358   ┆ /carter/users/dlaub/.cache/poo… │\n",
       "│ NA20811 ┆ 24398971   ┆ /carter/users/dlaub/.cache/poo… │\n",
       "│ NA20768 ┆ 30019566   ┆ /carter/users/dlaub/.cache/poo… │\n",
       "└─────────┴────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigwig_table = (\n",
    "    pl.read_csv(bw_table_path)\n",
    "    .join(\n",
    "        pl.Series(bw_paths).to_frame(\"realpath\"),\n",
    "        left_on=\"path\",\n",
    "        right_on=pl.col(\"realpath\").str.split(\"/\").list.get(-1),\n",
    "    )\n",
    "    .drop(\"path\")\n",
    "    .rename({\"realpath\": \"path\"})\n",
    ")\n",
    "bigwig_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll need a BED file specifying what regions to include in the dataset. We can either specify a path or a polars DataFrame. We'll use [gvl.read_bedlike](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.read_bedlike) to conveniently read the BED file into memory and subset it to just the first 5 regions for this tutorial. The BED file provided corresponds to eGenes, sorted in descending order by their absolute sum of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>chrom</th><th>chromStart</th><th>chromEnd</th><th>name</th><th>score</th><th>strand</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>str</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>&quot;chr22&quot;</td><td>41699499</td><td>41699499</td><td>&quot;ENSG00000167077&quot;</td><td>null</td><td>&quot;+&quot;</td></tr><tr><td>&quot;chr22&quot;</td><td>42835412</td><td>42835412</td><td>&quot;ENSG00000100266&quot;</td><td>null</td><td>&quot;-&quot;</td></tr><tr><td>&quot;chr22&quot;</td><td>20858983</td><td>20858983</td><td>&quot;ENSG00000099940&quot;</td><td>null</td><td>&quot;+&quot;</td></tr><tr><td>&quot;chr22&quot;</td><td>20707691</td><td>20707691</td><td>&quot;ENSG00000241973&quot;</td><td>null</td><td>&quot;-&quot;</td></tr><tr><td>&quot;chr22&quot;</td><td>49918167</td><td>49918167</td><td>&quot;ENSG00000184164&quot;</td><td>null</td><td>&quot;+&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌───────┬────────────┬──────────┬─────────────────┬───────┬────────┐\n",
       "│ chrom ┆ chromStart ┆ chromEnd ┆ name            ┆ score ┆ strand │\n",
       "│ ---   ┆ ---        ┆ ---      ┆ ---             ┆ ---   ┆ ---    │\n",
       "│ str   ┆ i64        ┆ i64      ┆ str             ┆ f64   ┆ str    │\n",
       "╞═══════╪════════════╪══════════╪═════════════════╪═══════╪════════╡\n",
       "│ chr22 ┆ 41699499   ┆ 41699499 ┆ ENSG00000167077 ┆ null  ┆ +      │\n",
       "│ chr22 ┆ 42835412   ┆ 42835412 ┆ ENSG00000100266 ┆ null  ┆ -      │\n",
       "│ chr22 ┆ 20858983   ┆ 20858983 ┆ ENSG00000099940 ┆ null  ┆ +      │\n",
       "│ chr22 ┆ 20707691   ┆ 20707691 ┆ ENSG00000241973 ┆ null  ┆ -      │\n",
       "│ chr22 ┆ 49918167   ┆ 49918167 ┆ ENSG00000184164 ┆ null  ┆ +      │\n",
       "└───────┴────────────┴──────────┴─────────────────┴───────┴────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bed = gvl.read_bedlike(bed_path)[:5]\n",
    "bed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to write the dataset.\n",
    "\n",
    "We'll instantiate a [gvl.BigWigs](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.BigWigs) object here, which has alternative constructors in case we don't want to use a table. We also name this track as \"depth\" (as in read depth) so we can manage different transformations of the track data or provide multiple tracks for the same samples. Later, we'll add a transformed track for $\\log_2(\\text{CPM}+1)$ to see this in action.\n",
    "\n",
    "We also will pass `max_jitter` as 128. This will allow random jittering of the sequences and tracks up to 128 bp in either direction. When we open the dataset later it will use the maximum amount of jitter by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-03 16:05:37.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mWriting dataset to geuvadis.chr22.gvl\u001b[0m\n",
      "\u001b[32m2024-09-03 16:05:37.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._variants._records\u001b[0m:\u001b[36mread_pvar\u001b[0m:\u001b[36m432\u001b[0m - \u001b[1mReading .pvar file...\u001b[0m\n",
      "\u001b[32m2024-09-03 16:05:39.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._variants._records\u001b[0m:\u001b[36mread_pvar\u001b[0m:\u001b[36m440\u001b[0m - \u001b[1mFinished reading .pvar file.\u001b[0m\n",
      "\u001b[32m2024-09-03 16:05:40.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mUsing 451 samples.\u001b[0m\n",
      "\u001b[32m2024-09-03 16:05:40.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m143\u001b[0m - \u001b[1mWriting genotypes.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6305db7a168541ed98db9df1ff73c626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-03 16:05:41.080\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36m_read_variants_chunk\u001b[0m:\u001b[36m381\u001b[0m - \u001b[34m\u001b[1mregion length 34024\u001b[0m\n",
      "\u001b[32m2024-09-03 16:05:41.081\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36m_read_variants_chunk\u001b[0m:\u001b[36m387\u001b[0m - \u001b[34m\u001b[1mread genotypes\u001b[0m\n",
      "\u001b[32m2024-09-03 16:05:49.475\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36m_read_variants_chunk\u001b[0m:\u001b[36m398\u001b[0m - \u001b[34m\u001b[1mget haplotype region ilens\u001b[0m\n",
      "\u001b[32m2024-09-03 16:05:49.507\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36m_read_variants_chunk\u001b[0m:\u001b[36m404\u001b[0m - \u001b[34m\u001b[1maverage haplotype length 34010.851219512195\u001b[0m\n",
      "\u001b[32m2024-09-03 16:05:49.509\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36m_read_variants_chunk\u001b[0m:\u001b[36m407\u001b[0m - \u001b[34m\u001b[1mmax missing length -726\u001b[0m\n",
      "\u001b[32m2024-09-03 16:05:49.510\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36m_read_variants_chunk\u001b[0m:\u001b[36m417\u001b[0m - \u001b[34m\u001b[1msparsify genotypes\u001b[0m\n",
      "\u001b[32m2024-09-03 16:05:49.577\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36m_read_variants_chunk\u001b[0m:\u001b[36m427\u001b[0m - \u001b[34m\u001b[1mmaximum needed length 33298\u001b[0m\n",
      "\u001b[32m2024-09-03 16:05:49.579\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36m_read_variants_chunk\u001b[0m:\u001b[36m428\u001b[0m - \u001b[34m\u001b[1mminimum needed length 33038\u001b[0m\n",
      "\u001b[32m2024-09-03 16:05:49.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mWriting regions.\u001b[0m\n",
      "\u001b[32m2024-09-03 16:05:49.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m163\u001b[0m - \u001b[1mWriting BigWig intervals.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b204e268a9034a7084b52f0ac999347c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-03 16:05:50.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mFinished writing.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "gvl.write(\n",
    "    path=ds_path,\n",
    "    bed=bed,\n",
    "    variants=variants,\n",
    "    bigwigs=gvl.BigWigs.from_table(name=\"depth\", table=bigwig_table),\n",
    "    length=2**15,\n",
    "    max_jitter=128,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that [gvl.write](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.write) will also automatically use the intersection of samples from source files. In this case, they are perfectly matched to each other. But, if we had used PLINK files for the full 3,202 samples from the 1000 Genomes Project then it would have identified and used the 451 intersecting samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-03 16:05:55.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset\u001b[0m:\u001b[36m_open\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1m\n",
      "GVL store geuvadis.chr22.gvl\n",
      "Is subset: False\n",
      "# of regions: 5\n",
      "# of samples: 451\n",
      "Original region length: 32,768\n",
      "Max jitter: 128\n",
      "Has genotypes: True\n",
      "Has tracks: ['depth']\u001b[0m\n",
      "\u001b[32m2024-09-03 16:05:55.645\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mgenvarloader._dataset\u001b[0m:\u001b[36mopen\u001b[0m:\u001b[36m235\u001b[0m - \u001b[33m\u001b[1mGenotypes found but no reference genome provided. This is required to reconstruct haplotypes. No reference or haplotype sequences can be returned by this dataset instance.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ds = gvl.Dataset.open(ds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't provide a reference genome to a dataset that has genotypes, we will get an informative warning and the dataset will never provide haplotypes. Let's go ahead and specify a reference genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-03 16:05:56.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset\u001b[0m:\u001b[36m_open\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mLoading reference genome into memory. This typically has a modest memory footprint (a few GB) and greatly improves performance.\u001b[0m\n",
      "\u001b[32m2024-09-03 16:05:58.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset\u001b[0m:\u001b[36m_open\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1m\n",
      "GVL store geuvadis.chr22.gvl\n",
      "Is subset: False\n",
      "# of regions: 5\n",
      "# of samples: 451\n",
      "Original region length: 32,768\n",
      "Max jitter: 128\n",
      "Has genotypes: True\n",
      "Has tracks: ['depth']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ds = gvl.Dataset.open(ds_path, reference=reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that a reference genome is provided, haplotypes can be returned. We also are given some summary information about this dataset. Let's use the dataset to inspect a few sequences and tracks and seeing how we can adjust what is returned as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[b'G', b'T', b'G', ..., b'T', b'G', b'T'],\n",
       "        [b'C', b'C', b'A', ..., b'A', b'C', b'T']], dtype='|S1'),\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing into a GVL dataset corresponds to the raveled indices, so the 0-th index is the data for the first region and sample.\n",
    "\n",
    "Since this dataset has jitter enabled (the maximum amount by default), we will get different data each time we access it. We can disable jittering, but we will still get randomly shifted data for haplotypes that are longer than the output length due to indels. We can also provide a seed to the dataset for determinism, see [gvl.Dataset.with_settings](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.Dataset.with_settings).\n",
    "\n",
    "We also are receiving both haplotypes and tracks from the dataset, and they have an additional dimension for ploidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 32768), (2, 32768)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a.shape for a in ds[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can disable returning haplotypes and return reference sequences instead, and now the ploidy dimension will be gone. We can also see that disabling jitter will increase the sequence length to the maximum available. We could disable jittering without altering sequence length by slicing the them  on-the-fly with a transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33024,), (33024,)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_ds = ds.with_settings(jitter=0, return_sequences='reference')\n",
    "[a.shape for a in ref_ds[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also slice the dataset or use lists/arrays of indices to get batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 2, 32768), (10, 2, 32768)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a.shape for a in ds[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 2, 32768), (3, 2, 32768)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a.shape for a in ds[[0, 3, 999]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: pre-computing transformed tracks and saving them to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we would like to normalize the read depth across the dataset to account for library size. We could compute this on-the-fly, but GVL also offers a way to write this data back to disk to cache this computation and potentially improve performance. Note that this is the most technical part of this tutorial, so feel free to skip this and come back later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27256165, 43941108, 39687917, 22341838, 23258231])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_library_sizes = (\n",
    "    pl.Series(ds.samples)\n",
    "    .to_frame(\"sample\")\n",
    "    .join(bigwig_table, on=\"sample\", how=\"left\")[\"read_count\"]\n",
    "    .to_numpy()\n",
    ")\n",
    "sample_library_sizes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this step, we'll use [gvl.Dataset.write_transformed_track](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.Dataset.write_transformed_track) which expects a transform function to be given. From the docs:\n",
    "\n",
    "> The arguments given to the transform will be the dataset indices, region indices, and sample indices as numpy arrays and the tracks themselves as a [Ragged](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.Ragged) array with shape (regions, samples). The tracks must be a [Ragged](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.Ragged) array since regions may be different lengths to accomodate indels. This function should then return the transformed tracks as a [Ragged](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.Ragged) array with the same shape and lengths.\n",
    "\n",
    "Below, you can see an example of a transform of ragged data that uses Numba to accelerate the computation. Note that working with [Ragged](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.Ragged) arrays is generally not necessary with on-the-fly transformations, since the data is processed to be uniform length before any transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729ce174a1554edb8ae77a33f58c5fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@nb.njit(parallel=True, nogil=True, fastmath=True)\n",
    "def inner_transform(s_idx, data, offsets):\n",
    "    log_cpm = np.empty_like(data)\n",
    "    for i in nb.prange(len(offsets) - 1):\n",
    "        start = offsets[i]\n",
    "        end = offsets[i + 1]\n",
    "        sample = s_idx[i]\n",
    "        log_cpm[start:end] = np.log1p(\n",
    "            data[start:end] / sample_library_sizes[sample] * 1e6\n",
    "        )\n",
    "    return log_cpm\n",
    "\n",
    "\n",
    "def log_cpm(ds_idx, r_idx, s_idx, tracks: gvl.Ragged[np.float32]):\n",
    "    data = inner_transform(s_idx, tracks.data, tracks.offsets)\n",
    "    return gvl.Ragged.from_offsets(data, tracks.shape, tracks.offsets)\n",
    "\n",
    "\n",
    "ds = ds.write_transformed_track(\"lcpb\", \"depth\", log_cpm, overwrite=True, max_mem=4 * 2**30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-the-fly transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing you may have noticed is that the sequences are output as ASCII characters. We'll often need to either tokenize or one-hot encode them for machine learning models. We can do this on-the-fly with, for example, fast implementations from [SeqPro](https://github.com/ML4GLand/SeqPro), but in general arbitrary transformations can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_transform(haplotypes, tracks):\n",
    "    return sp.tokenize(haplotypes, dict(zip(sp.DNA.alphabet, range(4))), 4), tracks\n",
    "\n",
    "\n",
    "def ohe_transform(haplotypes, tracks):\n",
    "    return sp.DNA.ohe(haplotypes), tracks\n",
    "\n",
    "\n",
    "token_ds = ds.with_settings(transform=tokenize_transform)\n",
    "ohe_ds = ds.with_settings(transform=ohe_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 3, ..., 3, 1, 3],\n",
       "        [3, 3, 0, ..., 2, 2, 1]], dtype=int32),\n",
       " array([[[1, 0, 0, 0],\n",
       "         [0, 0, 0, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         ...,\n",
       "         [0, 0, 1, 0],\n",
       "         [0, 0, 1, 0],\n",
       "         [1, 0, 0, 0]],\n",
       " \n",
       "        [[0, 1, 0, 0],\n",
       "         [1, 0, 0, 0],\n",
       "         [0, 1, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 1],\n",
       "         [0, 0, 1, 0],\n",
       "         [0, 0, 1, 0]]], dtype=uint8))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ds[0][0], ohe_ds[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we're training a model and thus need to split our dataset. Let's create a subset of the dataset to the first 400 samples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GVL store geuvadis.chr22.gvl\n",
       "Is subset: True\n",
       "# of regions: 5\n",
       "# of samples: 400\n",
       "Original region length: 32,768\n",
       "Max jitter: 128\n",
       "Has genotypes: True\n",
       "Has tracks: ['depth', 'lcpb']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = ds.subset_to(samples=ds.samples[:400])\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now the dataset is marked as a subset and the # of samples has reduced from 451 to 400. Some other properties reflect these changes as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2255, 2000, (5, 451), (5, 400))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds), len(train_ds), ds.shape, train_ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting a dataset, it can be very useful to have indices mapping each sample to its region and sample in the full dataset. GVL datasets can return these by enabling `return_indices`. When this is enabled, three arrays are appended to each instance returned. Each corresponds to the full dataset, region, and sample indices respectively. For example, we can see that the 401st instance from the train dataset maps to the 451st instance in the full dataset aka the first region and second sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[b'G', b'A', b'C', ..., b'T', b'T', b'C'],\n",
       "        [b'C', b'T', b'G', ..., b'T', b'C', b'C']], dtype='|S1'),\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 1., 1.]], dtype=float32),\n",
       " array([451]),\n",
       " array([0]),\n",
       " array([1]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.with_settings(return_indices=True)[400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These indices can be used to index into additional data that has no sequence length. For example, if we wanted to predict RNA-seq counts instead of read depth, we could use it to index into a gene expression table of counts. Or if we were working with chromatin accessibility data, we could do the same with a table of peak counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed50fb79517d46c4a876c3be29736a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 2, 32768]), torch.Size([16, 2, 32768]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl = train_ds.to_dataloader(batch_size=64, shuffle=True)\n",
    "\n",
    "for batch in tqdm(train_dl):\n",
    "    pass\n",
    "\n",
    "batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, since GVL provides a map-style PyTorch Dataset it is compatible with distributed data parallel (DDP) for use across multiple GPUs or nodes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GVL",
   "language": "python",
   "name": "gvl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
