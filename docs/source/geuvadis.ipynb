{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genvarloader as gvl\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import seqpro as sp\n",
    "import pooch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenVarLoader Tutorial: Geuvadis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll see how to use GenVarLoader (GVL) to:\n",
    "1. Write a GVL dataset to disk\n",
    "2. Inspect the dataset\n",
    "3. Optional: write transformed versions of the tracks to disk\n",
    "4. Add on-the-fly transformations\n",
    "5. Obtain splits from the dataset\n",
    "6. Get a PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Geuvadis dataset is 451 individuals from the 1000 Genomes Project that have both whole genome sequencing and RNA-seq from blood samples. We'll see how to use GVL to get a high performance dataloader that yields haplotypes and tracks for training or running inference with sequence models. For the sake of this tutorial, we'll only work with chromosome 22 so everything can run in a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data from 'https://ftp.ensembl.org/pub/release-112/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.22.fa.gz' to file '/carter/users/dlaub/.cache/pooch/edfb24b9fee5f1060c26e092a696e447-Homo_sapiens.GRCh38.dna.chromosome.22.fa.gz'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 11.4M/11.4M [00:00<00:00, 13.7GB/s]\n",
      "Downloading data from 'doi:10.5281/zenodo.13656224/1kGP.chr22.pgen' to file '/carter/users/dlaub/.cache/pooch/1kGP.chr22.pgen'.\n",
      "100%|█████████████████████████████████████| 20.4M/20.4M [00:00<00:00, 26.3GB/s]\n",
      "Downloading data from 'doi:10.5281/zenodo.13656224/1kGP.chr22.psam' to file '/carter/users/dlaub/.cache/pooch/1kGP.chr22.psam'.\n",
      "100%|█████████████████████████████████████| 6.38k/6.38k [00:00<00:00, 9.92MB/s]\n",
      "Downloading data from 'doi:10.5281/zenodo.13656224/1kGP.chr22.pvar' to file '/carter/users/dlaub/.cache/pooch/1kGP.chr22.pvar'.\n",
      "100%|█████████████████████████████████████| 1.17G/1.17G [00:00<00:00, 1.35TB/s]\n",
      "Downloading data from 'doi:10.5281/zenodo.13656224/bw_chr22.tar.gz' to file '/carter/users/dlaub/.cache/pooch/c91b5d0155f29d91dddf23901e396774-bw_chr22.tar.gz'.\n",
      "100%|█████████████████████████████████████| 1.01G/1.01G [00:00<00:00, 1.30TB/s]\n",
      "Untarring contents of '/carter/users/dlaub/.cache/pooch/c91b5d0155f29d91dddf23901e396774-bw_chr22.tar.gz' to '/carter/users/dlaub/.cache/pooch/c91b5d0155f29d91dddf23901e396774-bw_chr22.tar.gz.untar'\n",
      "Downloading data from 'doi:10.5281/zenodo.13656224/bigwig_table.csv' to file '/carter/users/dlaub/.cache/pooch/943cc42de0153b9ace8f53b274ba7616-bigwig_table.csv'.\n",
      "0.00B [00:00, ?B/s]\n",
      "Downloading data from 'doi:10.5281/zenodo.13656224/chr22_egenes.bed' to file '/carter/users/dlaub/.cache/pooch/99949e386019e2a73aa81c2caf793b77-chr22_egenes.bed'.\n",
      "100%|█████████████████████████████████████| 7.26k/7.26k [00:00<00:00, 8.30MB/s]\n"
     ]
    }
   ],
   "source": [
    "# GRCh38 chromosome 22 sequence\n",
    "reference = pooch.retrieve(\n",
    "    url=\"https://ftp.ensembl.org/pub/release-112/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.22.fa.gz\",\n",
    "    known_hash=\"sha256:974f97ac8ef7ffae971b63b47608feda327403be40c27e391ee4a1a78b800df5\",\n",
    "    progressbar=True,\n",
    ")\n",
    "!gzip -dc {reference} | bgzip > {reference[:-3]}.bgz\n",
    "reference = reference[:-3] + \".bgz\"\n",
    "\n",
    "# PLINK 2 files\n",
    "variants = pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/1kGP.chr22.pgen\",\n",
    "    known_hash=\"md5:31aba970e35f816701b2b99118dfc2aa\",\n",
    "    progressbar=True,\n",
    "    fname=\"1kGP.chr22.pgen\",\n",
    ")\n",
    "pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/1kGP.chr22.psam\",\n",
    "    known_hash=\"md5:eefa7aad5acffe62bf41df0a4600129c\",\n",
    "    progressbar=True,\n",
    "    fname=\"1kGP.chr22.psam\",\n",
    ")\n",
    "pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/1kGP.chr22.pvar\",\n",
    "    known_hash=\"md5:5f922af91c1a2f6822e2f1bb4469d12b\",\n",
    "    progressbar=True,\n",
    "    fname=\"1kGP.chr22.pvar\",\n",
    ")\n",
    "\n",
    "# BigWigs and sample ID mapping\n",
    "bw_paths = pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/bw_chr22.tar.gz\",\n",
    "    known_hash=\"md5:14bf72e9e9d3e2318d07315c4a2675fb\",\n",
    "    progressbar=True,\n",
    "    processor=pooch.Untar(),\n",
    ")\n",
    "bw_table_path = pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/bigwig_table.csv\",\n",
    "    known_hash=\"md5:7fe7c55b61c7dfa66cfd0a49336f3b08\",\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "# BED\n",
    "bed_path = pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/chr22_egenes.bed\",\n",
    "    known_hash=\"md5:ccb55548e4ddd416d50dbe6638459421\",\n",
    "    progressbar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the GVL dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll specify a path to store the dataset, which is a directory (like Zarr stores if you're familiar with those)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = \"geuvadis.chr22.gvl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need a table or dictionary specifying the sample names for each BigWig. We'll use a table here, which must have at least have columns `sample` and `path` as seen below. The join is added here to update the paths to match the actual download paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sample</th><th>read_count</th><th>path</th></tr><tr><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;HG00285&quot;</td><td>26025190</td><td>&quot;/carter/users/dlaub/.cache/poo…</td></tr><tr><td>&quot;NA20514&quot;</td><td>26304702</td><td>&quot;/carter/users/dlaub/.cache/poo…</td></tr><tr><td>&quot;NA19152&quot;</td><td>27759874</td><td>&quot;/carter/users/dlaub/.cache/poo…</td></tr><tr><td>&quot;NA20581&quot;</td><td>23282559</td><td>&quot;/carter/users/dlaub/.cache/poo…</td></tr><tr><td>&quot;NA18910&quot;</td><td>17936970</td><td>&quot;/carter/users/dlaub/.cache/poo…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────────┬────────────┬─────────────────────────────────┐\n",
       "│ sample  ┆ read_count ┆ path                            │\n",
       "│ ---     ┆ ---        ┆ ---                             │\n",
       "│ str     ┆ i64        ┆ str                             │\n",
       "╞═════════╪════════════╪═════════════════════════════════╡\n",
       "│ HG00285 ┆ 26025190   ┆ /carter/users/dlaub/.cache/poo… │\n",
       "│ NA20514 ┆ 26304702   ┆ /carter/users/dlaub/.cache/poo… │\n",
       "│ NA19152 ┆ 27759874   ┆ /carter/users/dlaub/.cache/poo… │\n",
       "│ NA20581 ┆ 23282559   ┆ /carter/users/dlaub/.cache/poo… │\n",
       "│ NA18910 ┆ 17936970   ┆ /carter/users/dlaub/.cache/poo… │\n",
       "└─────────┴────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigwig_table = (\n",
    "    pl.read_csv(bw_table_path)\n",
    "    .join(\n",
    "        pl.Series(bw_paths).to_frame(\"realpath\"),\n",
    "        left_on=\"path\",\n",
    "        right_on=pl.col(\"realpath\").str.split(\"/\").list.get(-1),\n",
    "    )\n",
    "    .drop(\"path\")\n",
    "    .rename({\"realpath\": \"path\"})\n",
    ")\n",
    "bigwig_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll need a BED file specifying what regions to include in the dataset. We can either specify a path or a polars DataFrame. We'll use `gvl.read_bedlike` to conveniently read the BED file into memory and subset it to just the first 5 regions for this tutorial. The BED file provided corresponds to eGenes, sorted in descending order by their absolute sum of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>chrom</th><th>chromStart</th><th>chromEnd</th><th>name</th><th>score</th><th>strand</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>str</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>&quot;chr22&quot;</td><td>41699499</td><td>41699499</td><td>&quot;ENSG00000167077&quot;</td><td>null</td><td>&quot;+&quot;</td></tr><tr><td>&quot;chr22&quot;</td><td>42835412</td><td>42835412</td><td>&quot;ENSG00000100266&quot;</td><td>null</td><td>&quot;-&quot;</td></tr><tr><td>&quot;chr22&quot;</td><td>20858983</td><td>20858983</td><td>&quot;ENSG00000099940&quot;</td><td>null</td><td>&quot;+&quot;</td></tr><tr><td>&quot;chr22&quot;</td><td>20707691</td><td>20707691</td><td>&quot;ENSG00000241973&quot;</td><td>null</td><td>&quot;-&quot;</td></tr><tr><td>&quot;chr22&quot;</td><td>49918167</td><td>49918167</td><td>&quot;ENSG00000184164&quot;</td><td>null</td><td>&quot;+&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌───────┬────────────┬──────────┬─────────────────┬───────┬────────┐\n",
       "│ chrom ┆ chromStart ┆ chromEnd ┆ name            ┆ score ┆ strand │\n",
       "│ ---   ┆ ---        ┆ ---      ┆ ---             ┆ ---   ┆ ---    │\n",
       "│ str   ┆ i64        ┆ i64      ┆ str             ┆ f64   ┆ str    │\n",
       "╞═══════╪════════════╪══════════╪═════════════════╪═══════╪════════╡\n",
       "│ chr22 ┆ 41699499   ┆ 41699499 ┆ ENSG00000167077 ┆ null  ┆ +      │\n",
       "│ chr22 ┆ 42835412   ┆ 42835412 ┆ ENSG00000100266 ┆ null  ┆ -      │\n",
       "│ chr22 ┆ 20858983   ┆ 20858983 ┆ ENSG00000099940 ┆ null  ┆ +      │\n",
       "│ chr22 ┆ 20707691   ┆ 20707691 ┆ ENSG00000241973 ┆ null  ┆ -      │\n",
       "│ chr22 ┆ 49918167   ┆ 49918167 ┆ ENSG00000184164 ┆ null  ┆ +      │\n",
       "└───────┴────────────┴──────────┴─────────────────┴───────┴────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bed = gvl.read_bedlike(bed_path)[:5]\n",
    "bed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to write the dataset. We'll instantiate a gvl.BigWigs object here, which has alternative constructors in case we didn't want to use a table. We also name this track as \"depth\" (as in read depth) so we can manage different transformations of the track data or provide multiple tracks for the same samples. Later, we'll add a transformed track for $\\log_2(\\text{CPM}+1)$ to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-03 15:10:38.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mWriting dataset to geuvadis.chr22.gvl\u001b[0m\n",
      "\u001b[32m2024-09-03 15:10:38.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mFound existing GVL store, overwriting.\u001b[0m\n",
      "\u001b[32m2024-09-03 15:10:38.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._variants._records\u001b[0m:\u001b[36mread_pvar\u001b[0m:\u001b[36m432\u001b[0m - \u001b[1mReading .pvar file...\u001b[0m\n",
      "\u001b[32m2024-09-03 15:10:40.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._variants._records\u001b[0m:\u001b[36mread_pvar\u001b[0m:\u001b[36m440\u001b[0m - \u001b[1mFinished reading .pvar file.\u001b[0m\n",
      "\u001b[32m2024-09-03 15:10:41.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mUsing 451 samples.\u001b[0m\n",
      "\u001b[32m2024-09-03 15:10:41.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m143\u001b[0m - \u001b[1mWriting genotypes.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9303128cd81c4cbca61652d5924da8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-03 15:10:42.083\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36m_read_variants_chunk\u001b[0m:\u001b[36m381\u001b[0m - \u001b[34m\u001b[1mregion length 34024\u001b[0m\n",
      "\u001b[32m2024-09-03 15:10:42.084\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36m_read_variants_chunk\u001b[0m:\u001b[36m387\u001b[0m - \u001b[34m\u001b[1mread genotypes\u001b[0m\n",
      "\u001b[32m2024-09-03 15:10:50.604\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36m_read_variants_chunk\u001b[0m:\u001b[36m398\u001b[0m - \u001b[34m\u001b[1mget haplotype region ilens\u001b[0m\n",
      "\u001b[32m2024-09-03 15:10:50.648\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36m_read_variants_chunk\u001b[0m:\u001b[36m404\u001b[0m - \u001b[34m\u001b[1maverage haplotype length 34010.851219512195\u001b[0m\n",
      "\u001b[32m2024-09-03 15:10:50.650\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36m_read_variants_chunk\u001b[0m:\u001b[36m407\u001b[0m - \u001b[34m\u001b[1mmax missing length -726\u001b[0m\n",
      "\u001b[32m2024-09-03 15:10:50.651\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36m_read_variants_chunk\u001b[0m:\u001b[36m417\u001b[0m - \u001b[34m\u001b[1msparsify genotypes\u001b[0m\n",
      "\u001b[32m2024-09-03 15:10:50.717\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36m_read_variants_chunk\u001b[0m:\u001b[36m427\u001b[0m - \u001b[34m\u001b[1mmaximum needed length 33298\u001b[0m\n",
      "\u001b[32m2024-09-03 15:10:50.719\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36m_read_variants_chunk\u001b[0m:\u001b[36m428\u001b[0m - \u001b[34m\u001b[1mminimum needed length 33038\u001b[0m\n",
      "\u001b[32m2024-09-03 15:10:51.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mWriting regions.\u001b[0m\n",
      "\u001b[32m2024-09-03 15:10:51.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m163\u001b[0m - \u001b[1mWriting BigWig intervals.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754f772ffe564bf0bc36f0a6d9c9170f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-03 15:10:51.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mFinished writing.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "gvl.write(\n",
    "    path=ds_path,\n",
    "    bed=bed,\n",
    "    variants=variants,\n",
    "    bigwigs=gvl.BigWigs.from_table(name=\"depth\", table=bigwig_table),\n",
    "    length=2**15,\n",
    "    max_jitter=128,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `gvl.write` will also automatically use the intersection of samples from source files. In this case, they are perfectly matched to each other. But, if we had used PLINK files for the full 3,202 samples from the 1000 Genomes Project then it would have identified and used the 451 intersecting samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-03 15:10:51.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset\u001b[0m:\u001b[36m_open\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1m\n",
      "GVL store geuvadis.chr22.gvl\n",
      "Is subset: False\n",
      "# of regions: 5\n",
      "# of samples: 451\n",
      "Original region length: 32,768\n",
      "Max jitter: 128\n",
      "Has genotypes: True\n",
      "Has tracks: ['depth']\u001b[0m\n",
      "\u001b[32m2024-09-03 15:10:51.838\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mgenvarloader._dataset\u001b[0m:\u001b[36mopen\u001b[0m:\u001b[36m235\u001b[0m - \u001b[33m\u001b[1mGenotypes found but no reference genome provided. This is required to reconstruct haplotypes. No reference or haplotype sequences can be returned by this dataset instance.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ds = gvl.Dataset.open(ds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't provide a reference genome to a dataset that has genotypes, we will get an informative warning and the dataset will never provide haplotypes. Let's go ahead and specify a reference genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-03 15:10:51.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset\u001b[0m:\u001b[36m_open\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mLoading reference genome into memory. This typically has a modest memory footprint (a few GB) and greatly improves performance.\u001b[0m\n",
      "\u001b[32m2024-09-03 15:10:53.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset\u001b[0m:\u001b[36m_open\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1m\n",
      "GVL store geuvadis.chr22.gvl\n",
      "Is subset: False\n",
      "# of regions: 5\n",
      "# of samples: 451\n",
      "Original region length: 32,768\n",
      "Max jitter: 128\n",
      "Has genotypes: True\n",
      "Has tracks: ['depth']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ds = gvl.Dataset.open(ds_path, reference=reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that a reference genome is provided, haplotypes can be returned. We also are given some summary information to get a sense of what is available in this dataset. Let's use the dataset to inspect a few sequences and tracks and seeing how we can adjust what is returned as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[b'A', b'A', b'T', ..., b'C', b'T', b'G'],\n",
       "        [b'T', b'A', b'T', ..., b'G', b'G', b'C']], dtype='|S1'),\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing into a GVL dataset corresponding to the raveled indices, so the 0-th index is the data for the first region and sample.\n",
    "\n",
    "Since this dataset has jitter enabled (the maximum amount by default), we will get different data each time we access it. We can disable jittering, but we will still get randomly shifted data for haplotypes that are longer than the output length due to indels.\n",
    "\n",
    "We also are receiving both haplotypes and tracks from the dataset, and they have an additional dimension for ploidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 32768), (2, 32768)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a.shape for a in ds[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can disable returning haplotypes and return reference sequences instead, and now the ploidy dimension will be gone. We can also see that disabling jitter will increase the sequence length to the maximum available. We could disable jittering without altering sequence length by slicing the them  on-the-fly with a transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33024,), (33024,)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_ds = ds.with_settings(jitter=0, return_sequences='reference')\n",
    "[a.shape for a in ref_ds[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also slice the dataset or use lists/arrays of indices to get batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 2, 32768), (10, 2, 32768)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a.shape for a in ds[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 2, 32768), (3, 2, 32768)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a.shape for a in ds[[0, 3, 999]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: pre-computing transformed tracks and saving them to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we would like to normalize the read depth across the dataset to account for library size. We could compute this on-the-fly, but GVL also offers a way to write this data back to disk to cache this computation and potentially improve performance. Note that this is the most technical part of this tutorial, so feel free to skip this and come back later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26025190, 26304702, 27759874, 23282559, 17936970])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_library_sizes = (\n",
    "    pl.Series(ds.samples)\n",
    "    .to_frame(\"sample\")\n",
    "    .join(bigwig_table, on=\"sample\")[\"read_count\"]\n",
    "    .to_numpy()\n",
    ")\n",
    "sample_library_sizes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the transform, we'll use [`gvl.Dataset.write_transformed_track`](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.Dataset.write_transformed_track) which expects a transform function to be given. From the docs:\n",
    "\n",
    "> The arguments given to the transform will be the dataset indices, region indices, and sample indices as numpy arrays and the tracks themselves as a [`Ragged`](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.Ragged) array with shape (regions, samples). The tracks must be a [`Ragged`](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.Ragged) array since regions may be different lengths to accomodate indels. This function should then return the transformed tracks as a [`Ragged`](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.Ragged) array with the same shape and lengths.\n",
    "\n",
    "Below, you can see an example of a transform of ragged data that uses Numba to accelerate the computation. Note that working with Ragged data is generally not necessary with on-the-fly transformations, since the data is processed to be uniform length before any transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748ce637c5194772b0d8edde29c78402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@nb.njit(parallel=True, nogil=True, fastmath=True)\n",
    "def inner_transform(s_idx, data, offsets):\n",
    "    log_cpm = np.empty_like(data)\n",
    "    for i in nb.prange(len(offsets) - 1):\n",
    "        start = offsets[i]\n",
    "        end = offsets[i + 1]\n",
    "        sample = s_idx[i]\n",
    "        log_cpm[start:end] = np.log1p(\n",
    "            data[start:end] / sample_library_sizes[sample] * 1e6\n",
    "        )\n",
    "    return log_cpm\n",
    "\n",
    "\n",
    "def log_cpm(ds_idx, r_idx, s_idx, tracks: gvl.Ragged[np.float32]):\n",
    "    data = inner_transform(s_idx, tracks.data, tracks.offsets)\n",
    "    return gvl.Ragged.from_offsets(data, tracks.shape, tracks.offsets)\n",
    "\n",
    "\n",
    "ds = ds.write_transformed_track(\"lcpb\", \"depth\", log_cpm, overwrite=True, max_mem=4 * 2**30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-the-fly transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing you may have noticed is that the sequences are output as ASCII characters. We'll often need to either tokenize or one-hot encode them for machine learning models. We can do this on-the-fly with, for example, fast implementations from [SeqPro](https://github.com/ML4GLand/SeqPro), but in general arbitrary transformations can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_transform(haplotypes, tracks):\n",
    "    return sp.tokenize(haplotypes, dict(zip(sp.DNA.alphabet, range(4))), 4), tracks\n",
    "\n",
    "\n",
    "def ohe_transform(haplotypes, tracks):\n",
    "    return sp.DNA.ohe(haplotypes), tracks\n",
    "\n",
    "\n",
    "token_ds = ds.with_settings(transform=tokenize_transform)\n",
    "ohe_ds = ds.with_settings(transform=ohe_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 3, 0, ..., 1, 2, 1],\n",
       "        [0, 3, 3, ..., 1, 2, 1]], dtype=int32),\n",
       " array([[[0, 0, 1, 0],\n",
       "         [0, 0, 0, 1],\n",
       "         [1, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 1, 0, 0],\n",
       "         [0, 1, 0, 0],\n",
       "         [0, 0, 1, 0]],\n",
       " \n",
       "        [[0, 0, 1, 0],\n",
       "         [0, 1, 0, 0],\n",
       "         [0, 0, 0, 1],\n",
       "         ...,\n",
       "         [0, 0, 0, 1],\n",
       "         [0, 1, 0, 0],\n",
       "         [0, 1, 0, 0]]], dtype=uint8))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ds[0][0], ohe_ds[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we're training a model and thus need to split our dataset. Let's create a subset of the dataset to the first 400 samples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GVL store geuvadis.chr22.gvl\n",
       "Is subset: True\n",
       "# of regions: 5\n",
       "# of samples: 400\n",
       "Original region length: 32,768\n",
       "Max jitter: 128\n",
       "Has genotypes: True\n",
       "Has tracks: ['depth', 'lcpb']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = ds.subset_to(samples=ds.samples[:400])\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now the dataset is marked as a subset and the # of samples has reduced from 451 to 400. Some other properties reflect these changes as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2255, 2000, (5, 451), (5, 400))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds), len(train_ds), ds.shape, train_ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 2, 32768]), torch.Size([128, 2, 32768]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl = train_ds.to_dataloader(batch_size=128, shuffle=True)\n",
    "batch = next(iter(train_dl))\n",
    "batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since GVL provides a map-style PyTorch Dataset, this approach is compatible with distributed data parallel (DDP) for use across multiple GPUs or nodes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GVL",
   "language": "python",
   "name": "gvl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
